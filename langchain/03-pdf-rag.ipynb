{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`03-pdf-rag.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 문서 로딩(Loader)\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 2. 문서 나누기(Split)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 3. 문서 변환하기(Embed) => GPT를 사용하기 때문에 OpenAI 에서 제공하는 Embedding 사용\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 4. Embedding 저장하기 (Vector Store)\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load\n",
    "loader = PyMuPDFLoader('./data.pdf')\n",
    "docs = loader.load()\n",
    "print(f'문서 페이지 수: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f'분할된 청크의 수: {len(split_docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embed Setting\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Vectorstore\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 검색(retrieve) => vectorstore 내장 검색기\n",
    "# 6. 프롬프트 생성(prompt)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 7. LLM \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 8. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Retrieve\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prompting\n",
    "\n",
    "text = '''\n",
    "넌 질문-답변을 도와주는 AI 비서야.\n",
    "아래 제공되는 Context를 통해서 사용자 Question에 대해 답을 해줘야해.\n",
    "\n",
    "Context에는 직접적으로 없어도, 추론하거나 계산할 수 있는 답변은 최대한 만들어 봐.\n",
    "그리고 관련 내용을 말할때는, pdf 몇 페이지이서 찾은 내용인지도 말해줘.\n",
    "\n",
    "답은 적절히 \\n를 통해 문단을 나눠줘 한국어로 만들어 줘. \n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "\n",
    "# Answer:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "# 8. Chain\n",
    "chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = chain.invoke('삼성전자 관련 소식들을 다 가져와')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain (대화 내용 기억하는 체인)\n",
    "- Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are a Kind Good Chatbot '),\n",
    "        ('placeholder', '{chat_history}'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(chain, lambda h: history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = wrapped_chain.invoke(\n",
    "    {'input': 'Hello. My name is Yu.'},\n",
    "    config={'configurable': {'session_id': 715211}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans2 = wrapped_chain.invoke(\n",
    "    {'input': 'Any nickname recommendation?'},\n",
    "    config={'configurable': {'session_id': 715211}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans3 = wrapped_chain.invoke(\n",
    "    {'input': 'What is my name?'},\n",
    "    config={'configurable': {'session_id': 715211}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ConversationalRetrievalChain`\n",
    "- Question-Answering with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 페이지 수: 23\n",
      "분할된 청크의 수: 72\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ConversationalQA\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "# 1. Load\n",
    "loader = PyMuPDFLoader('./data.pdf')\n",
    "docs = loader.load()\n",
    "print(f'문서 페이지 수: {len(docs)}')\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f'분할된 청크의 수: {len(split_docs)}')\n",
    "\n",
    "# 3. Embed Setting\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 4. Vectorstore\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "question_prompt_template = '''\n",
    "    Given a chat history and the latest user question\n",
    "    which might reference context in the chat history,\n",
    "    formulate a standalone question which can be understood\n",
    "    without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\n",
    "'''\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', question_prompt_template),\n",
    "        ('placeholder', '{chat_history}'),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Retrieve\n",
    "retriever = create_history_aware_retriever(\n",
    "    llm, vectorstore.as_retriever(), question_prompt,\n",
    ")\n",
    "\n",
    "# 6. Prompting\n",
    "prompt = '''\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. Use three sentences maximum and keep the answer concise.\n",
    "Answer in Korean.\n",
    "# Context:\n",
    "{context}\n",
    "'''\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "conv_qa_chain = create_retrieval_chain(\n",
    "    retriever, qa_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성 관련 자료 다 가져와\n",
      "삼성 가우스는 온디바이스에서 작동 가능한 생성 AI 모델로, 언어, 코드, 이미지의 3개 모델로 구성되어 있습니다. 이 모델은 안전한 데이터를 통해 학습되어 개인정보를 침해하지 않으며, 사용자 정보 유출 위험이 없는 장점을 가지고 있습니다. 또한, 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획이며, 이를 통해 삼성 스마트폰이 경쟁력을 갖출 것으로 예상됩니다.\n",
      "방금 말한거에서 AI 이름만 가져와\n",
      "삼성 가우스\n",
      "\n",
      "안녕하세요! 궁금한 것이 있으시면 무엇이든 물어보세요. 도와드리겠습니다.\n",
      "\n",
      "언제든지 도움이 필요하시면 또 문의해 주세요. 좋은 하루 되세요!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_msg = input('물어봐: ')\n",
    "    ans = conv_qa_chain.invoke(\n",
    "        {\n",
    "            'input': user_msg,\n",
    "            'chat_history': chat_history\n",
    "        }\n",
    "    )\n",
    "    chat_history += [\n",
    "        HumanMessage(user_msg),\n",
    "        AIMessage(ans['answer'])\n",
    "    ]\n",
    "    print(user_msg)\n",
    "    print(ans['answer'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
